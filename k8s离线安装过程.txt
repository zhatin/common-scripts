
修改/etc/hosts文件
10.224.47.161   master01
10.224.47.162   worker01
10.224.47.163   worker02
10.224.47.164   worker03

设置时区
timedatectl set-timezone Asia/Shanghai

设置主机名
hostnamectl set-hostname master01
hostnamectl set-hostname worker01
hostnamectl set-hostname worker02
hostnamectl set-hostname worker03

修改/etc/selinux/config
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
setenforce 0

关闭防火墙
systemctl disable firewalld
systemctl stop firewalld

设置内核参数
vi /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

sysctl --system

关闭交换分区
swapoff -a
sed -e '/swap/s/^/#/g' -i /etc/fstab

上传Docker离线安装包docker.tar.gz
安装Docker
rpm -Uvh --replacefiles --replacepkgs --nodeps --force *.rpm
systemctl enable docker
systemctl start docker

修改Docker参数
vi /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}

systemctl daemon-reload
systemctl restart docker

yumdownloader --resolve kubelet kubeadm kubectl

上传k8s离线安装包k8s.tar.gz
安装kubeadm
rpm -Uvh --replacefiles --replacepkgs --nodeps --force *.rpm

systemctl enable kubelet
systemctl start kubelet

导入镜像
docker load -i coredns.tar 
docker load -i etcd.tar 
docker load -i flannel.tar
docker load -i kube-apiserver.tar 
docker load -i kube-controller-manager.tar 
docker load -i kube-proxy.tar 
docker load -i kube-scheduler.tar 
docker load -i pause.tar

find . -name "*.tar.gz" -print0 | xargs -0 -I {} docker load -i {} 

初始化master 
kubeadm init --pod-network-cidr=10.244.0.0/16
-----------------------------------------------------------
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.224.47.161:6443 --token pe6cd8.il4wx3aw5rcvtfhx \
    --discovery-token-ca-cert-hash sha256:5098c4dbec46eadb0800943ee8eb751e19e402e49a5d4ff3fb88eaf5105e0f80 
-----------------------------------------------------------

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

安装Flannel
kubectl apply -f kube-flannel.yml

加入Worker
kubeadm join 10.224.47.161:6443 --token pe6cd8.il4wx3aw5rcvtfhx \
    --discovery-token-ca-cert-hash sha256:5098c4dbec46eadb0800943ee8eb751e19e402e49a5d4ff3fb88eaf5105e0f80 

设置Worker的ROLES标签
kubectl label node worker01 node-role.kubernetes.io/worker=worker
kubectl label node worker02 node-role.kubernetes.io/worker=worker
kubectl label node worker03 node-role.kubernetes.io/worker=worker

------------------------------------------
[root@master01 ~]# kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
master01   Ready    master   15h   v1.18.2
worker01   Ready    worker   15h   v1.18.2
worker02   Ready    worker   15h   v1.18.2
worker03   Ready    worker   15h   v1.18.2
[root@master01 ~]# 
------------------------------------------

------------------------------------------
[root@master01 ~]# kubectl get pods -A -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE       NOMINATED NODE   READINESS GATES
kube-system   coredns-66bff467f8-2hht5           1/1     Running   0          16h   10.244.1.2      worker01   <none>           <none>
kube-system   coredns-66bff467f8-7zkx2           1/1     Running   0          16h   10.244.0.2      master01   <none>           <none>
kube-system   etcd-master01                      1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-apiserver-master01            1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-controller-manager-master01   1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-flannel-ds-amd64-hfnl4        1/1     Running   0          16h   10.224.47.163   worker02   <none>           <none>
kube-system   kube-flannel-ds-amd64-lc84v        1/1     Running   0          16h   10.224.47.164   worker03   <none>           <none>
kube-system   kube-flannel-ds-amd64-vdgf5        1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-flannel-ds-amd64-xd9j5        1/1     Running   0          16h   10.224.47.162   worker01   <none>           <none>
kube-system   kube-proxy-4gnrv                   1/1     Running   0          16h   10.224.47.162   worker01   <none>           <none>
kube-system   kube-proxy-6xfzt                   1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-proxy-kq2t8                   1/1     Running   0          16h   10.224.47.164   worker03   <none>           <none>
kube-system   kube-proxy-znxqx                   1/1     Running   0          16h   10.224.47.163   worker02   <none>           <none>
kube-system   kube-scheduler-master01            1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
[root@master01 ~]# 
------------------------------------------

上传Metallb安装包文件metallb.tar.gz
导入镜像文件

docker load -i controller.tar
docker load -i speaker.tar

修改参数
kubectl edit configmap -n kube-system kube-proxy

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
ipvs:
  strictARP: true
  
编写config.yaml
vi config.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 10.224.47.165-10.224.47.166

安装Metallb
kubectl apply -f namespace.yaml
kubectl apply -f metallb.yaml
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
kubectl apply -f config.yaml

编辑参数
kubectl edit configmap/config -n metallb-system

上传Glusterfs文件包gluster.tar.gz
导入镜像文件

docker load -i gluster.tar
docker load -i heketi.tar

在Workers上安装glusterfs-fuse
rpm -Uvh --replacefiles --replacepkgs --nodeps --force *.rpm

vi topology.json

{
  "clusters": [
    {
      "nodes": [
        {
          "node": {
            "hostnames": {
              "manage": [
                "worker01"
              ],
              "storage": [
                "10.224.47.162"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "worker02"
              ],
              "storage": [
                "10.224.47.163"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "worker03"
              ],
              "storage": [
                "10.224.47.164"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        }
      ]
    }
  ]
}		

编辑yaml文件，修改api版本和增加spec/selector

部署gluster
kubectl create namespace heketi

擦除磁盘
wipefs --all --force /dev/sdb

在worker上加载内核模块
modprobe dm_thin_pool

vi /etc/modules-load.d/glusterfs.conf

dm_thin_pool

如果出现Can't open /dev/sdb exclusively.  Mounted filesystem?
 echo 1 > /sys/block/sdb/device/delete

删除gk-deploy中--show-all参数后运行
./gk-deploy -g -v -n heketi --admin-key heketi-glusterfs -y 

----------------------------------------
Determining heketi service URL ... OK

heketi is now running and accessible via http://10.244.2.32:8080 . To run
administrative commands you can install 'heketi-cli' and use it as follows:

  # heketi-cli -s http://10.244.2.32:8080 --user admin --secret '<ADMIN_KEY>' cluster list

You can find it at https://github.com/heketi/heketi/releases . Alternatively,
use it from within the heketi pod:

  # /usr/bin/kubectl -n heketi exec -it <HEKETI_POD> -- heketi-cli -s http://localhost:8080 --user admin --secret '<ADMIN_KEY>' cluster list

For dynamic provisioning, create a StorageClass similar to this:

---
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: glusterfs-storage
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://10.244.2.32:8080"


Deployment complete!

[root@master01 deploy]# 
----------------------------------------

新建StorageClass
[root@master01 deploy]# vi storage-class.yaml
---
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: glusterfs-storage
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://10.244.2.32:8080"
  restuser: "admin"
  restuserkey: "heketi-glusterfs"

kubectl create -f storage-class.yaml

如果需要修改reseturl
kubectl replace -f storage-class.yaml --force 

[root@master01 deploy]# kubectl get storageclass
NAME                PROVISIONER               RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
glusterfs-storage   kubernetes.io/glusterfs   Delete          Immediate           false                  15s
[root@master01 deploy]#

新建PersistentVolumeClaim
vi persistent-volumeclaim.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: gluster-test
 annotations:
   volume.beta.kubernetes.io/storage-class: glusterfs-storage
spec:
 accessModes:
  - ReadWriteOnce
 resources:
   requests:
     storage: 5Gi

kubectl create -f persistent-volumeclaim.yaml 

persistentvolumeclaim/gluster-test created
[root@master01 deploy]# kubectl get pvc
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
gluster-test   Bound    pvc-957238b5-517f-4017-a55a-a321b1676486   5Gi        RWO            glusterfs-storage   8s
[root@master01 deploy]#
[root@master01 deploy]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS        REASON   AGE
pvc-957238b5-517f-4017-a55a-a321b1676486   5Gi        RWO            Delete           Bound    default/gluster-test   glusterfs-storage            47s
[root@master01 deploy]#

测试Gluster
vi nginx.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-test-pod
  labels:
    name: nginx-test-pod
spec:
  containers:
  - name: nginx-test-pod
    image: nginx
    imagePullPolicy: IfNotPresent
    ports:
    - name: web
      containerPort: 80
    volumeMounts:
    - name: gluster-vol-test
      mountPath: /usr/share/nginx/html
  volumes:
  - name: gluster-vol-test
    persistentVolumeClaim:
      claimName: gluster-test

kubectl create -f nginx.yaml

vi nginx-deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        imagePullPolicy: IfNotPresent
        ports:
        - name: web
          containerPort: 80
        volumeMounts:
        - name: gluster-vol-test
          mountPath: /usr/share/nginx/html
      volumes:
      - name: gluster-vol-test
        persistentVolumeClaim:
          claimName: gluster-test

kubectl create -f nginx-deploy.yaml

kubectl expose deployment nginx-deployment --port=80 --type=LoadBalancer

[root@master01 deploy]# kubectl get svc -o wide
NAME                                                     TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE     SELECTOR
details                                                  ClusterIP      10.98.113.165    <none>          9080/TCP       2d19h   app=details
glusterfs-dynamic-957238b5-517f-4017-a55a-a321b1676486   ClusterIP      10.105.118.147   <none>          1/TCP          57m     <none>
kubernetes                                               ClusterIP      10.96.0.1        <none>          443/TCP        3d20h   <none>
nginx-deployment                                         LoadBalancer   10.96.239.82     10.224.47.166   80:30811/TCP   3m27s   app=nginx
productpage                                              ClusterIP      10.96.221.151    <none>          9080/TCP       2d19h   app=productpage
ratings                                                  ClusterIP      10.100.183.15    <none>          9080/TCP       2d19h   app=ratings
reviews                                                  ClusterIP      10.100.220.182   <none>          9080/TCP       2d19h   app=reviews
[root@master01 deploy]#

[root@localhost ~]# curl http://10.224.47.166
Hello World from GlusterFS!!!
[root@localhost ~]#

[root@master01 deploy]# kubectl get pods                 
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-8556f9b6fd-52rmp       2/2     Running   2          2d19h
nginx-test-pod                    1/1     Running   0          13s
productpage-v1-559cb4c69f-wkbw5   2/2     Running   2          2d19h
ratings-v1-66bc766cf4-sbvf5       2/2     Running   2          2d19h
reviews-v1-bcf9667f9-4c2s5        2/2     Running   2          2d19h
reviews-v2-7846b549c-mxt5q        2/2     Running   2          2d19h
reviews-v3-68c569cb87-xs7lh       2/2     Running   2          2d19h
[root@master01 deploy]#

[root@master01 deploy]# kubectl exec -ti nginx-test-pod -- /bin/bash
root@nginx-test-pod:/# cd /usr/share/nginx/html/
root@nginx-test-pod:/usr/share/nginx/html# echo 'Hello World from GlusterFS!!!' > index.html
root@nginx-test-pod:/usr/share/nginx/html# ls
index.html
root@nginx-test-pod:/usr/share/nginx/html# exit
exit
[root@master01 deploy]#

上传Istio安装安装文件和镜像包istio-image.tar.gz
导入镜像文件

docker load -i citadel.tar 
docker load -i coredns-plugin.tar 
docker load -i galley.tar 
docker load -i grafana.tar 
docker load -i mixer.tar 
docker load -i node-agent-k8s.tar 
docker load -i pilot.tar 
docker load -i proxyv2.tar 
docker load -i sidecar_injector.tar 
docker load -i prometheus.tar
docker load -i jaeger.tar
docker load -i kiali.tar

istioctl operator init
kubectl create ns istio-system

istio profile dump demo
-------------------------------------
vi demo.yaml

apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  namespace: istio-system
  name: example-istiocontrolplane
spec:
  profile: demo
  
kubectl apply -f demo.yaml

--------------------------------------------------
[root@master01 ~]# kubectl get svc -n istio-system
NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                                                                                                                      AGE
grafana                     ClusterIP      10.106.125.123   <none>          3000/TCP                                                                                                                                     98m
istio-egressgateway         ClusterIP      10.105.226.46    <none>          80/TCP,443/TCP,15443/TCP                                                                                                                     98m
istio-ingressgateway        LoadBalancer   10.110.198.166   10.224.47.165   15020:30732/TCP,80:32155/TCP,443:32631/TCP,15029:31461/TCP,15030:30922/TCP,15031:31468/TCP,15032:31650/TCP,31400:31367/TCP,15443:30773/TCP   98m
istio-pilot                 ClusterIP      10.103.30.159    <none>          15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP                                                                                     98m
istiod                      ClusterIP      10.103.48.155    <none>          15012/TCP,443/TCP                                                                                                                            98m
jaeger-agent                ClusterIP      None             <none>          5775/UDP,6831/UDP,6832/UDP                                                                                                                   98m
jaeger-collector            ClusterIP      10.96.15.188     <none>          14267/TCP,14268/TCP,14250/TCP                                                                                                                98m
jaeger-collector-headless   ClusterIP      None             <none>          14250/TCP                                                                                                                                    98m
jaeger-query                ClusterIP      10.103.55.178    <none>          16686/TCP                                                                                                                                    98m
kiali                       ClusterIP      10.96.151.89     <none>          20001/TCP                                                                                                                                    98m
prometheus                  ClusterIP      10.103.41.21     <none>          9090/TCP                                                                                                                                     98m
tracing                     ClusterIP      10.105.195.80    <none>          80/TCP                                                                                                                                       98m
zipkin                      ClusterIP      10.109.240.158   <none>          9411/TCP                                                                                                                                     98m
[root@master01 ~]#
--------------------------------------------------
自动注入
kubectl label namespace default istio-injection=enabled

使用手动注入（因Flannel与Istio的兼容问题，自动注入失败）
istioctl kube-inject -f bookinfo.yaml | kubectl apply -f -

打开Kiali
istioctl dashboard kiali

安装MySQL

[root@master01 mysql]# echo -n "mysql-test-secret-passwd" | base64
bXlzcWwtdGVzdC1zZWNyZXQtcGFzc3dk
[root@master01 mysql]#

vi secret.yaml

---
apiVersion: v1
kind: Secret
metadata:
  name: mysql-test-secrets
type: Opaque
data:
  ROOT_PASSWORD: bXlzcWwtdGVzdC1zZWNyZXQtcGFzc3dk

kubectl apply -f secret.yaml

vi pvc.yaml

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-test-disk
  annotations:
    volume.beta.kubernetes.io/storage-class: glusterfs-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

kubectl apply -f pvc.yaml

vi deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-test-deployment
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql:5
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3306
          volumeMounts:
            - mountPath: "/var/lib/mysql"
              subPath: "mysql"
              name: mysql-data
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-test-secrets
                  key: ROOT_PASSWORD
      volumes:
        - name: mysql-data
          persistentVolumeClaim:
            claimName: mysql-test-disk

kubectl apply -f deploy.yaml

vi svc.yaml

---
apiVersion: v1
kind: Service
metadata:
  name: mysql-test-service
spec:
  selector:
    app: mysql
  ports:
  - protocol: TCP
    port: 3306
    targetPort: 3306

kubectl apply -f svc.yaml

安装Apollo
初始化数据库
kubectl exec -it mysql-test-deployment-5f6c7599d9-ldstq -- mysql -u root -pmysql-test-secret-passwd < apolloconfigdb.sql
kubectl exec -it mysql-test-deployment-5f6c7599d9-ldstq -- mysql -u root -pmysql-test-secret-passwd < apolloportaldb.sql

修改yaml后
kubectl apply -f service-apollo-config-server-dev.yaml
kubectl apply -f service-apollo-admin-server-dev.yaml
kubectl apply -f service-apollo-portal-server.yaml

部署demo
kubectl apply -f deploy.yaml，或者istioctl kube-inject -f deploy.yaml | kubectl apply -f - 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: apollo-demo-deployment
  labels:
    app: apollo-demo-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: apollo-demo-app
  template:
    metadata:
      labels:
        app: apollo-demo-app
    spec:
      containers:
      - name: apollo-demo-app
        image: apollo-demo:0.0.1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: apollo-demo-service
spec:
  selector:
    app: apollo-demo-app
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30008
  type: LoadBalancer

安装redis
上传gcr-redis.tar

kubectl create -f redis-master.yaml
kubectl create -f redis-sentinel-service.yaml
kubectl create -f redis-controller.yaml
kubectl create -f redis-sentinel-controller.yaml

安装minio

如果PVC无法删除
kubectl describe pvc PVC_NAME | grep Finalizers
kubectl patch pvc PVC_NAME -p '{"metadata":{"finalizers": []}}' --type=merge

安装Spinnaker

apiVersion: v1
kind: ConfigMap
metadata:
  name: halyard-config-map
data:
  config: |
    halyard:
      halconfig:
        directory: /Users/spinnaker/.hal
    spinnaker:
      config:
        input:
          endpoint: http://minio:9000
          accessKeyId: "minio"
          secretAccessKey: "minio123"
          bucket: halconfig
          enablePathStyleAccess: true
    secrets:
      vault:
        enabled: false
        url: https://vault.url
        path: example
        role: example
        authMethod: KUBERNETES


kubectl -n spinnaker apply -f halyard.yml

kubectl -n spinnaker exec -it halyard-0 -- /bin/bash

hal config provider kubernetes enable --no-validate
hal config provider kubernetes account add spinnaker   --provider-version v2   --only-spinnaker-managed true   --service-account true   --namespaces spinnaker --no-validate
hal config deploy edit   --type distributed   --account-name spinnaker   --location spinnaker --no-validate

hal config features edit --artifacts true --no-validate
hal config features edit --artifacts-rewrite true --no-validate
hal config artifact http enable --no-validate

hal config storage s3 edit \
--bucket spinnaker \
--endpoint http://minio.default:9000 \
--access-key-id minio \
--secret-access-key minio123 \
--path-style-access true \
--no-validate

hal config storage edit --type s3

mkdir -p /home/spinnaker/.hal/default/{profiles,service-settings}

vi /home/spinnaker/.hal/default/profiles/gate-local.yml

server:
  servlet:
    context-path: /api/v1

vi /home/spinnaker/.hal/default/service-settings/gate.yml

healthEndpoint: /api/v1/health

echo "spinnaker.s3.versioning: false" > /home/spinnaker/.hal/default/profiles/front50-local.yml

hal version bom 1.20.2 -q -o yaml

hal config version edit --version 1.20.2 --no-validate

docker pull docker.io/armory/clouddriver:2.19.8
docker pull docker.io/armory/echo:2.19.8
docker pull docker.io/armory/fiat:2.19.6
docker pull docker.io/armory/front50:2.19.6
docker pull docker.io/armory/gate:2.19.5
docker pull docker.io/armory/igor:2.19.6
docker pull docker.io/armory/orca:2.19.9
docker pull docker.io/armory/rosco:2.19.6
docker pull docker.io/armory/deck:2.19.7
docker pull docker.io/armory/dinghy:2.19.5
docker pull docker.io/armory/terraformer:1.0.6
docker pull docker.io/armory/kayenta:2.19.5
docker pull docker.io/armory/monitoring-daemon:0.16.1-7d506f0-rc1
#docker pull docker.io/armory/monitoring-third-party:0.16.1-7d506f0-rc1（not exist）
docker pull docker.io/armory/redis:2


version: 2.19.8-rc.1
timestamp: "2020-04-22 01:49:47"
services:
  clouddriver:
    commit: 2bd55acb
    version: 2.19.8
  echo:
    commit: 43e1966a
    version: 2.19.8
  fiat:
    commit: e7d5efa3
    version: 2.19.6
  front50:
    commit: 32cc7a7c
    version: 2.19.6
  gate:
    commit: 5ea58df0
    version: 2.19.5
  igor:
    commit: 67f5ae20
    version: 2.19.6
  orca:
    commit: be0f8e7a
    version: 2.19.9
  rosco:
    commit: e168a011
    version: 2.19.6
  deck:
    commit: 4f6b2719
    version: 2.19.7
  dinghy:
    commit: ef444037
    version: 2.19.5
  terraformer:
    commit: f3edd3da
    version: 1.0.6
  kayenta:
    commit: fa1521ae
    version: 2.19.5
  monitoring-daemon:
    version: 0.16.1-7d506f0-rc1
  monitoring-third-party:
    version: 0.16.1-7d506f0-rc1
dependencies:
  redis:
    version: 2:2.8.4-2
artifactSources:
  dockerRegistry: docker.io/armory

hal deploy apply

openssl req \
 -newkey rsa:2048 \
 -nodes -sha256 \
 -x509 -days 365 \
 -keyout registry.key \
 -out registry.crt


kubectl create secret tls registry-tls \
    --key registry.key \
    --cert registry.crt


docker run -p 8084:8084 -p 9000:9000 \
    --name halyard --rm \
    -v ~/.hal:/home/spinnaker/.hal \
    -d \
    armory/halyard-armory:1.9.1

docker exec -it halyard bash

kubectl get storageclass
kubectl patch storageclass glusterfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

services:
  echo:
    version: 2.12.1-20200507020017
    commit: 7b04fb3d80dacba05dad29b754615a546168c8cc
  clouddriver:
    version: 6.8.2-20200518071229
    commit: f6709294361ac4459bf63f2064d43d699e4630e5
  deck:
    version: 3.1.1-20200518071229
    commit: e0f33be10ce4d2cf79ea7f66f31a6059ba3bc7ab
  fiat:
    version: 1.11.0-20200428134356
    commit: c88008c3a1359c445eee8cff23184e5484e472a9
  front50:
    version: 0.23.1-20200509020016
    commit: 4cbc3b136ac7cc200f7a58040e7d352390d38a31
  gate:
    version: 1.16.0-20200501020017
    commit: c9975aa819f06115a7a27ddf8156b487712cd980
  igor:
    version: 1.10.0-20200428134356
    commit: 1c1d7d69689b46cc6483ef23281a810ea458e51c
  kayenta:
    version: 0.15.1-20200509020016
    commit: f3af8422684b1c95eced83020e14c8e27ed7b466
  orca:
    version: 2.14.0-20200502020017
    commit: e2b32d9cc794b30048f502728f8126eae227eeec
  rosco:
    version: 0.19.0-20200428134356
    commit: 32481f1ed3532ac802cb9dc9cff719a480f63a20
  defaultArtifact: {}
  monitoring-third-party:
    version: 0.17.0-20200501020017
    commit: 226f54ffe8a49df95af290bcd57aa0c2f225ed36
  monitoring-daemon:
    version: 0.17.0-20200501020017
    commit: 226f54ffe8a49df95af290bcd57aa0c2f225ed36
dependencies:
  redis:
    version: 2:2.8.4-2
  consul:
    version: 0.7.5
  vault:
    version: 0.7.0
artifactSources:
  debianRepository: https://dl.bintray.com/spinnaker-releases/debians
  dockerRegistry: gcr.io/spinnaker-marketplace
  googleImageProject: marketplace-spinnaker-release
  gitPrefix: https://github.com/spinnaker

docker pull gcr.io/spinnaker-marketplace/clouddriver:6.8.2-20200518071229
docker pull gcr.io/spinnaker-marketplace/echo:2.12.1-20200507020017
docker pull gcr.io/spinnaker-marketplace/fiat:1.11.0-20200428134356
docker pull gcr.io/spinnaker-marketplace/front50:0.23.1-20200509020016
docker pull gcr.io/spinnaker-marketplace/gate:1.16.0-20200501020017
docker pull gcr.io/spinnaker-marketplace/igor:1.10.0-20200428134356
docker pull gcr.io/spinnaker-marketplace/orca:2.14.0-20200502020017
docker pull gcr.io/spinnaker-marketplace/rosco:0.19.0-20200428134356
docker pull gcr.io/spinnaker-marketplace/deck:3.1.1-20200518071229
docker pull gcr.io/spinnaker-marketplace/kayenta:0.15.1-20200509020016
docker pull gcr.io/spinnaker-marketplace/monitoring-daemon:0.17.0-20200501020017
docker pull gcr.io/spinnaker-marketplace/monitoring-third-party:0.17.0-20200501020017
docker pull gcr.io/kubernetes-spinnaker/redis-cluster:v2
docker pull gcr.io/spinnaker-marketplace/consul:0.7.5
docker pull gcr.io/spinnaker-marketplace/vault:0.7.0

docker save gcr.io/spinnaker-marketplace/clouddriver:6.8.2-20200518071229 > clouddriver.tar
docker save gcr.io/spinnaker-marketplace/echo:2.12.1-20200507020017 > echo.tar
docker save gcr.io/spinnaker-marketplace/fiat:1.11.0-20200428134356 > fiat.tar
docker save gcr.io/spinnaker-marketplace/front50:0.23.1-20200509020016 > front50.tar
docker save gcr.io/spinnaker-marketplace/gate:1.16.0-20200501020017 > gate.tar
docker save gcr.io/spinnaker-marketplace/igor:1.10.0-20200428134356 > igor.tar
docker save gcr.io/spinnaker-marketplace/orca:2.14.0-20200502020017 > orca.tar
docker save gcr.io/spinnaker-marketplace/rosco:0.19.0-20200428134356 > rosco.tar
docker save gcr.io/spinnaker-marketplace/deck:3.1.1-20200518071229 > deck.tar
docker save gcr.io/spinnaker-marketplace/kayenta:0.15.1-20200509020016 > kayenta.tar
docker save gcr.io/kubernetes-spinnaker/redis-cluster:v2 > redis.tar

docker load -i clouddriver.tar
docker load -i echo.tar
docker load -i fiat.tar
docker load -i front50.tar
docker load -i gate.tar
docker load -i igor.tar
docker load -i orca.tar
docker load -i rosco.tar
docker load -i deck.tar
docker load -i kayenta.tar
docker load -i redis.tar


kubectl delete svc spin-clouddriver -n spinnaker 
kubectl delete svc spin-deck -n spinnaker 
kubectl delete svc spin-echo -n spinnaker 
kubectl delete svc spin-front50 -n spinnaker 
kubectl delete svc spin-gate -n spinnaker 
kubectl delete svc spin-orca -n spinnaker 
kubectl delete svc spin-redis -n spinnaker 
kubectl delete svc spin-rosco -n spinnaker 

kubectl delete deploy spin-clouddriver -n spinnaker 
kubectl delete deploy spin-deck -n spinnaker 
kubectl delete deploy spin-echo -n spinnaker 
kubectl delete deploy spin-front50 -n spinnaker 
kubectl delete deploy spin-gate -n spinnaker 
kubectl delete deploy spin-orca -n spinnaker 
kubectl delete deploy spin-redis -n spinnaker 
kubectl delete deploy spin-rosco -n spinnaker 

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: spin-ingress
  labels:
    app: spin
    cluster: spin-ingress
  annotations:
    kubernetes.io/ingress.class: istio
spec:
  rules:
  - 
    # host: spin.k8s.test
    # ^ If we have other things running in our cluster, we should uncomment this line and specify a valid DNS name
    http:
      paths:
      - backend:
          serviceName: spin-deck
          servicePort: 9000
        path: /
      - backend:
          serviceName: spin-gate
          servicePort: 8084
        path: /api/v1

kubectl -n spinnaker expose service spin-gate --type LoadBalancer \
  --port 80,443 \
  --target-port 8084 \
  --name spin-gate-public

kubectl -n spinnaker expose service spin-deck --type LoadBalancer \
  --port 80,443 \
  --target-port 9000 \
  --name spin-deck-public

kubectl -n spinnaker get svc spin-gate-public -o jsonpath='{.status.loadBalancer.ingress[0].ip}'
kubectl -n spinnaker get svc spin-gate-public -o jsonpath='{.status.loadBalancer.ingress[0].ip}'

hal config security api edit --override-base-url http://10.224.47.169
hal config security ui edit --override-base-url http://10.224.47.170

升级Istio 1.6.0

docker pull quay.io/kiali/kiali:v1.18
docker pull docker.io/prom/prometheus:v2.15.1
docker pull docker.io/istio/proxyv2:1.6.0
docker pull docker.io/jaegertracing/all-in-one:1.16
docker pull docker.io/istio/pilot:1.6.0
docker pull grafana/grafana:6.5.2
docker pull docker.io/istio/mixer:1.6.0
docker pull docker.io/istio/install-cni:1.6.0
docker pull docker.io/istio/operator:1.6.0

docker save quay.io/kiali/kiali:v1.18 > kiali.tar
docker save docker.io/prom/prometheus:v2.15.1 > prometheus.tar
docker save docker.io/istio/proxyv2:1.6.0 > proxyv2.tar
docker save docker.io/jaegertracing/all-in-one:1.16 > jaeger.tar
docker save docker.io/istio/pilot:1.6.0 > pilot.tar
docker save grafana/grafana:6.5.2 > grafana.tar
docker save docker.io/istio/mixer:1.6.0 > mixer.tar
docker save docker.io/istio/install-cni:1.6.0 > install-cni.tar
docker save docker.io/istio/operator:1.6.0 > operator.tar

docker load -i kiali.tar
docker load -i prometheus.tar
docker load -i proxyv2.tar
docker load -i jaeger.tar
docker load -i pilot.tar
docker load -i grafana.tar
docker load -i mixer.tar
docker load -i install-cni.tar
docker load -i operator.tar

集群二
kubeadm init --pod-network-cidr=10.233.0.0/16 --service-cidr=10.112.0.0/12

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.224.47.165:6443 --token rczs7y.8afkzmhk6l4ok7x1 \
    --discovery-token-ca-cert-hash sha256:36c1d18c4ff06a442a36416dc34bd63a7cf69e97345feefd80c672547da5eaed 
[root@masterb01 rpms]# 

kubectl label node workerb01 node-role.kubernetes.io/worker=worker

Clean up
If you used disposable servers for your cluster, for testing, you can switch those off and do no further clean up. You can use kubectl config delete-cluster to delete your local references to the cluster.

However, if you want to deprovision your cluster more cleanly, you should first drain the node and make sure that the node is empty, then deconfigure the node.

Remove the node
Talking to the control-plane node with the appropriate credentials, run:

kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
Then, on the node being removed, reset all kubeadm installed state:

kubeadm reset
The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:

iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
If you want to reset the IPVS tables, you must run the following command:

ipvsadm -C
If you wish to start over simply run kubeadm init or kubeadm join with the appropriate arguments.

Clean up the control plane
You can use kubeadm reset on the control plane host to trigger a best-effort clean up.

See the kubeadm reset reference documentation for more information about this subcommand and its options.

第三方服务账号
kubectl get --raw /api/v1 | jq '.resources[] | select(.name | index("serviceaccounts/token"))'

多网络共享控制平面

kubectl create ns istio-system
kubectl create secret generic cacerts -n istio-system --from-file=samples/certs/ca-cert.pem --from-file=samples/certs/ca-key.pem --from-file=samples/certs/root-cert.pem --from-file=samples/certs/cert-chain.pem

istioctl manifest apply -f install/kubernetes/operator/examples/multicluster/values-istio-multicluster-primary.yaml

kubectl get pods -n istio-system

kubectl create namespace istio-system
kubectl create secret generic cacerts -n istio-system \
    --from-file=samples/certs/ca-cert.pem \
    --from-file=samples/certs/ca-key.pem \
    --from-file=samples/certs/root-cert.pem \
    --from-file=samples/certs/cert-chain.pem

Main cluster

vi istio-main-cluster.yaml

apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  values:
    # selfSigned is required if Citadel is enabled, that is when values.global.istiod.enabled is false.
    security:
      selfSigned: false

    global:
      multiCluster:
        clusterName: main0
      network: network1

      # Mesh network configuration. This is optional and may be omitted if
      # all clusters are on the same network.
      meshNetworks:
        network1:
          endpoints:
          # Always use Kubernetes as the registry name for the main cluster in the mesh network configuration
          - fromRegistry: Kubernetes
          gateways:
          - registry_service_name: istio-ingressgateway.istio-system.svc.cluster.local
            port: 443

        network2:
          endpoints:
          - fromRegistry: remote0
          gateways:
          - registry_service_name: istio-ingressgateway.istio-system.svc.cluster.local
            port: 443

      # Use the existing istio-ingressgateway.
      meshExpansion:
        enabled: true

istioctl  manifest apply -f istio-main-cluster.yaml  

istioctl  manifest apply -f istio-main-cluster.yaml \
  --set addonComponents.grafana.enabled=true \
  --set addonComponents.kiali.enabled=true \
  --set addonComponents.prometheus.enabled=true \
  --set addonComponents.tracing.enabled=true \
  --set values.gateways.istio-egressgateway.enabled=true 

$ export ISTIOD_REMOTE_EP=$(kubectl  -n istio-system get svc istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
$ echo "ISTIOD_REMOTE_EP is ${ISTIOD_REMOTE_EP}"

Remote cluster

vi istio-remote0-cluster.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  values:
    global:
      # The remote cluster's name and network name must match the values specified in the
      # mesh network configuration of the main cluster.
      multiCluster:
        clusterName: remote0
      network: network2

      # Replace ISTIOD_REMOTE_EP with the the value of ISTIOD_REMOTE_EP set earlier.
      remotePilotAddress: 10.224.47.168

  ## The istio-ingressgateway is not required in the remote cluster if both clusters are on
  ## the same network. To disable the istio-ingressgateway component, uncomment the lines below.
  #
  # components:
  #  ingressGateways:
  #  - name: istio-ingressgateway
  #    enabled: false

istioctl  manifest apply -f istio-remote0-cluster.yaml

vi cluster-aware-gateway.yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: cluster-aware-gateway
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: tls
      protocol: TLS
    tls:
      mode: AUTO_PASSTHROUGH
    hosts:
    - "*.local"

kubectl  apply -f cluster-aware-gateway.yaml

istioctl x create-remote-secret --context=kubernetes-admin165@kubernetes --name remote0 | \
    kubectl apply -f - --context=kubernetes-admin161@kubernetes

kubectl create  namespace sample
kubectl label  namespace sample istio-injection=enabled

Remote Cluster
kubectl create  -f samples/helloworld/helloworld.yaml -l app=helloworld -n sample
kubectl create  -f samples/helloworld/helloworld.yaml -l version=v2 -n sample

Main Cluster
kubectl create -f samples/helloworld/helloworld.yaml -l app=helloworld -n sample
kubectl create -f samples/helloworld/helloworld.yaml -l version=v1 -n sample

kubectl get pod -n sample

kubectl apply  -f samples/sleep/sleep.yaml -n sample

kubectl exec  -it -n sample -c sleep $(kubectl get pod  -n sample -l app=sleep -o jsonpath='{.items[0].metadata.name}') -- curl helloworld.sample:5000/hello
kubectl exec  -it -n sample -c sleep $(kubectl get pod  -n sample -l app=sleep -o jsonpath='{.items[0].metadata.name}') -- curl helloworld.sample:5000/hello

kubectl  -n sample get pod -l app=sleep -o name | cut -f2 -d'/' | \
    xargs -I{} istioctl  -n sample proxy-config endpoints {} --cluster "outbound|5000||helloworld.sample.svc.cluster.local"

Get Service CIDR
echo '{"apiVersion":"v1","kind":"Service","metadata":{"name":"tst"},"spec":{"clusterIP":"1.1.1.1","ports":[{"port":443}]}}' | kubectl apply -f - 2>&1 | sed 's/.*valid IPs is //'

vi kiali-secret.yaml

apiVersion: v1
kind: Secret
metadata:
  name: kiali
  namespace: istio-system
  labels:
    app: kiali
type: Opaque
data:
  username: YWRtaW4=
  passphrase: YWRtaW4=

设置locality标签
kubectl label node worker01 topology.kubernetes.io/region=us-east-2
kubectl label node worker02 topology.kubernetes.io/region=us-east-2
kubectl label node worker03 topology.kubernetes.io/region=us-east-2

kubectl label node workerb01 topology.kubernetes.io/region=us-west-2


多网络复制控制平面+VM

kubectl create namespace istio-system
kubectl create secret generic cacerts -n istio-system \
    --from-file=samples/certs/ca-cert.pem \
    --from-file=samples/certs/ca-key.pem \
    --from-file=samples/certs/root-cert.pem \
    --from-file=samples/certs/cert-chain.pem

istioctl manifest apply \
    -f install/kubernetes/operator/examples/multicluster/values-istio-multicluster-gateways.yaml \
    --set values.global.meshExpansion.enabled=true

kubectl get svc -n istio-system istiocoredns -o jsonpath={.spec.clusterIP}

vi dns-config.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           upstream
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
    global:53 {
        errors
        cache 30
        forward . 10.111.246.12:53
    }

kubectl apply -f dns-config.yaml

Cluster1

kubectl create namespace foo
kubectl label namespace foo istio-injection=enabled
kubectl apply -n foo -f samples/sleep/sleep.yaml

kubectl get -n foo pod -l app=sleep -o jsonpath={.items..metadata.name}

Cluster2

kubectl create namespace bar
kubectl label namespace bar istio-injection=enabled
kubectl apply -n bar -f samples/httpbin/httpbin.yaml

kubectl get svc --selector=app=istio-ingressgateway \
    -n istio-system -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}'

Cluster1

vi httpbin-svc.yaml

apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: httpbin-bar
spec:
  hosts:
  # must be of form name.namespace.global
  - httpbin.bar.global
  # Treat remote cluster services as part of the service mesh
  # as all clusters in the service mesh share the same root of trust.
  location: MESH_INTERNAL
  ports:
  - name: http1
    number: 8000
    protocol: http
  resolution: DNS
  addresses:
  # the IP address to which httpbin.bar.global will resolve to
  # must be unique for each remote service, within a given cluster.
  # This address need not be routable. Traffic for this IP will be captured
  # by the sidecar and routed appropriately.
  - 240.0.0.2
  endpoints:
  # This is the routable address of the ingress gateway in cluster2 that
  # sits in front of sleep.foo service. Traffic from the sidecar will be
  # routed to this address.
  - address: 10.224.47.170
    ports:
      http1: 15443 # Do not change this port value

kubectl apply -n foo -f httpbin-svc.yaml
kubectl exec sleep-8f795f47d-sfvth -n foo -c sleep -- curl -I httpbin.bar.global:8000/headers


run in cluster main0
istioctl experimental add-to-mesh external-service vmhttp 10.224.47.167 http:8080 -n vm
#istioctl x remove-from-mesh external-service vmhttp -n vm

Admiral
Cluster1

kubectl delete envoyfilter istio-multicluster-ingressgateway -n istio-system
export ADMIRAL_HOME=./admiral-install-v0.9
$ADMIRAL_HOME/scripts/install_admiral.sh $ADMIRAL_HOME
mkdir /tmp
$ADMIRAL_HOME/scripts/cluster-secret.sh ~/.kube/config-161  ~/.kube/config-161 admiral

$ADMIRAL_HOME/scripts/install_sample_services.sh $ADMIRAL_HOME
kubectl edit deploy greeting -n sample

kubectl get ServiceEntry -n admiral-sync
kubectl get ServiceEntry default.greeting.global-se -n admiral-sync -o yaml

Cluster2

kubectl delete envoyfilter istio-multicluster-ingressgateway -n istio-system
export ADMIRAL_HOME=./admiral-install-v0.9

kubectl apply -f $ADMIRAL_HOME/yaml/remotecluster.yaml
$ADMIRAL_HOME/scripts/cluster-secret.sh ~/.kube/config-161  ~/.kube/config-165 admiral

kubectl apply -f $ADMIRAL_HOME/yaml/remotecluster_sample.yaml
kubectl edit deploy greeting -n sample

Run in Cluster1
kubectl get serviceentry default.greeting.global-se -n admiral-sync -o yaml

kubectl exec --namespace=sample -it $(kubectl get pod -l "app=webapp" --namespace=sample -o jsonpath='{.items[0].metadata.name}') -c webapp -- curl -v http://default.greeting.global

kubectl apply -f $ADMIRAL_HOME/yaml/gtp.yaml

设置locality标签
kubectl label node worker01 topology.kubernetes.io/region=us-east-2
kubectl label node worker02 topology.kubernetes.io/region=us-east-2
kubectl label node worker03 topology.kubernetes.io/region=us-east-2

kubectl label node workerb01 topology.kubernetes.io/region=us-west-2

修改MTU
kubectl patch configmap/Calico-config -n kube-system --type merge \
  -p '{"data":{"veth_mtu": "1440"}}'

部署registry
openssl req -newkey rsa:2048 -nodes -sha256 -keyout certs/domain.key -x509 -days 365 -out certs/domain.crt

docker run -d \
  --restart=always \
  --name registry \
  -v /root/certs:/certs \
  -v /var/lib/registry:/var/lib/registry \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
  -p 443:443 \
  registry:2

配置docker client
mkdir -p /etc/docker/certs.d/registry.test.com

cp certs/domain.crt /etc/docker/certs.d/registry.test.com/ca.crt

docker tag istio/install-cni:1.5.6 registry.test.com/istio/install-cni:1.5.6
docker tag istio/proxyv2:1.5.6 registry.test.com/istio/proxyv2:1.5.6
docker tag istio/app_sidecar:1.5.6 registry.test.com/istio/app_sidecar:1.5.6
docker tag istio/pilot:1.5.6 registry.test.com/istio/pilot:1.5.6
docker tag istio/mixer:1.5.6 registry.test.com/istio/mixer:1.5.6
docker tag istio/galley:1.5.6 registry.test.com/istio/galley:1.5.6
docker tag istio/sidecar_injector:1.5.6 registry.test.com/istio/sidecar_injector:1.5.6
docker tag istio/operator:1.5.6 registry.test.com/istio/operator:1.5.6
docker tag istio/node-agent-k8s:1.5.6 registry.test.com/istio/node-agent-k8s:1.5.6
docker tag istio/citadel:1.5.6 registry.test.com/istio/citadel:1.5.6

docker push registry.test.com/istio/install-cni:1.5.6
docker push registry.test.com/istio/proxyv2:1.5.6
docker push registry.test.com/istio/app_sidecar:1.5.6
docker push registry.test.com/istio/pilot:1.5.6
docker push registry.test.com/istio/mixer:1.5.6
docker push registry.test.com/istio/galley:1.5.6
docker push registry.test.com/istio/sidecar_injector:1.5.6
docker push registry.test.com/istio/operator:1.5.6
docker push registry.test.com/istio/node-agent-k8s:1.5.6
docker push registry.test.com/istio/citadel:1.5.6

curl --cacert certs/domain.crt https://registry.test.com/v2/_catalog 
curl --cacert certs/domain.crt https://registry.test.com/v2/bitnami/postgresql/tags/list

扩展lvm
fdisk /dev/sda
partprobe
pvcreate /dev/sda3
vgextend /dev/centos /dev/sda3
vgdisplay -v
lvextend -l+100%Free /dev/centos/root
xfs_growfs /dev/centos/root
df -h

使用Helm安装Mysql HA
helm install mydbcluster bitnami/mysql \
  --set global.imageRegistry=registry.test.com \
  --set global.storageClass=rook-ceph-block \
  --set image.registry=registry.test.com \
  --set image.tag=8.0.20-debian-10-r41 -n mysql

[root@localhost mysql]# helm status mydbcluster
NAME: mydbcluster
LAST DEPLOYED: Wed Jun 24 14:16:41 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Please be patient while the chart is being deployed

Tip:

  Watch the deployment status using the command: kubectl get pods -w --namespace default

Services:

  echo Master: mydbcluster-mysql.default.svc.cluster.local:3306
  echo Slave:  mydbcluster-mysql-slave.default.svc.cluster.local:3306

Administrator credentials:

  echo Username: root
  echo Password : $(kubectl get secret --namespace default mydbcluster-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode)

To connect to your database:

  1. Run a pod that you can use as a client:

      kubectl run mydbcluster-mysql-client --rm --tty -i --restart='Never' --image  registry.test.com/bitnami/mysql:8.0 --namespace default --command -- bash

  2. To connect to master service (read/write):

      mysql -h mydbcluster-mysql.default.svc.cluster.local -uroot -p my_database

  3. To connect to slave service (read-only):

      mysql -h mydbcluster-mysql-slave.default.svc.cluster.local -uroot -p my_database

To upgrade this helm chart:

  1. Obtain the password as described on the 'Administrator credentials' section and set the 'root.password' parameter as shown below:

      ROOT_PASSWORD=$(kubectl get secret --namespace default mydbcluster-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode)
      helm upgrade mydbcluster bitnami/mysql --set root.password=$ROOT_PASSWORD

WARNING: Rolling tag detected (bitnami/mysql:8.0), please note that it is strongly recommended to avoid using rolling tags in a production environment.
+info https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/




WARNING: Rolling tag detected (bitnami/mysql:8.0), please note that it is strongly recommended to avoid using rolling tags in a production environment.
+info https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/
[root@localhost mysql]# 


kubectl exec -it mydbcluster-mysql-client -n mysql -c mydbcluster-mysql-client  -- mysql  -u root -h mydbcluster-mysql.mysql.svc.cluster.local -pbmPer4ZGY5 < apolloconfigdb.sql

 kubectl scale --replicas=1 deploy apollo-demo-deployment

使用Helm安装redis-cluster（自动注入时，须手动删除creator）
helm install redis-cache bitnami/redis-cluster \
  --set image.registry=registry.test.com \
  --set image.repository=bitnami/redis-cluster \
  --set image.tag=6.0.5 \
  --set image.pullPolicy=Always 

现象
Istio 升级到 1.5.0 之后，新建命名空间，并开启 Istio sidecar 注入后，部署服务，会出现如下报错

MountVolume.SetUp failed for volume "istiod-ca-cert" : configmap "istio-ca-root-cert" not found
解决方法
kubectl rollout restart deployment istiod -n istio-system
参考
https://github.com/istio/istio/issues/22463

helm install redis-cache bitnami/redis -n redis \
  --set image.registry=registry.test.com \
  --set image.repository=bitnami/redis \
  --set image.tag=6.0.5-debian-10-r15 \
  --set image.pullPolicy=Always  

NAME: redis-cache
LAST DEPLOYED: Tue Jun 30 16:10:52 2020
NAMESPACE: redis
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **
Redis can be accessed via port 6379 on the following DNS names from within your cluster:

redis-cache-master.redis.svc.cluster.local for read/write operations
redis-cache-slave.redis.svc.cluster.local for read-only operations


To get your password run:

    export REDIS_PASSWORD=$(kubectl get secret --namespace redis redis-cache -o jsonpath="{.data.redis-password}" | base64 --decode)

To connect to your Redis server:

1. Run a Redis pod that you can use as a client:
   kubectl run --namespace redis redis-cache-client --rm --tty -i --restart='Never' \
    --env REDIS_PASSWORD=$REDIS_PASSWORD \
   --image registry.test.com/bitnami/redis:6.0.5 -- bash

2. Connect using the Redis CLI:
   redis-cli -h redis-cache-master -a $REDIS_PASSWORD
   redis-cli -h redis-cache-slave -a $REDIS_PASSWORD

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace redis svc/redis-cache-master 6379:6379 &
    redis-cli -h 127.0.0.1 -p 6379 -a $REDIS_PASSWORD


apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: apollo-gateway
spec:
  selector:
    istio: ingressgateway # use istio default ingress gateway
  servers:
  - port:
      number: 8070
      name: http
      protocol: HTTP
    hosts:
    - "apollo.test.com"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: apollo-web
spec:
  hosts:
  - "apollo.test.com"
  gateways:
  - apollo-gateway
  http:
  - route:
    - destination:
        port:
          number: 8070
        host: service-apollo-portal-server.apollo.svc.cluster.local


helm install apollo-service-dev -n apollo \
    --set configService.image.repository=registry.test.com/apolloconfig/apollo-configservice \
    --set adminService.image.repository=registry.test.com/apolloconfig/apollo-adminservice \
    --set configdb.host=10.104.221.153 \
    --set configdb.userName=root \
    --set configdb.password=bmPer4ZGY5 \
    apollo/apollo-service

NOTES:
Get meta service url for current release by running these commands:
  echo http://apollo-service-dev-apollo-configservice.apollo:8080

For local test use:
  export POD_NAME=$(kubectl get pods --namespace apollo -l "app=apollo-service-dev-apollo-configservice" -o jsonpath="{.items[0].metadata.name}")
  echo http://127.0.0.1:8080
  kubectl --namespace apollo port-forward $POD_NAME 8080:8080
[root@localhost helm]# 

helm install apollo-portal -n apollo \
    --set image.repository=registry.test.com/apolloconfig/apollo-portal \
    --set portaldb.host=10.104.221.153 \
    --set portaldb.userName=root \
    --set portaldb.password=bmPer4ZGY5 \
    --set config.envs="dev" \
    --set replicaCount=1 \
    --set config.metaServers.dev=http://apollo-service-dev-apollo-configservice.apollo:8080 \
    apollo/apollo-portal


NOTES:
Get apollo portal url by running these commands:
  export POD_NAME=$(kubectl get pods --namespace apollo -l "app=apollo-portal" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8070 to use your application"
  kubectl --namespace apollo port-forward $POD_NAME 8070:8070
[root@localhost helm]# 

kubectl exec sleep-8f795f47d-sfvth -n foo -c sleep -- curl -s apollo-demo-service.apollo-demo.svc.cluster.local:8080   

安装cert manager
helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version v0.15.1 \
  --set installCRDs=true \
  --set cainjector.image.repository=registry.test.com/jetstack/cert-manager-cainjector \
  --set image.repository=registry.test.com/jetstack/cert-manager-controller \
  --set webhook.image.repository=registry.test.com/jetstack/cert-manager-webhook

NOTES:
cert-manager has been deployed successfully!

In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).

More information on the different types of issuers and how to configure them
can be found in our documentation:

https://cert-manager.io/docs/configuration/

For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:

https://cert-manager.io/docs/usage/ingress/
[root@localhost cert-manager]# 

kubectl get configmap istio -n istio-system -o yaml | grep -o "mode: ALLOW_ANY" | uniq
kubectl get configmap istio -n istio-system -o yaml | sed 's/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g' | kubectl replace -n istio-system -f -
kubectl get configmap istio -n istio-system -o yaml | sed 's/mode: REGISTRY_ONLY/mode: ALLOW_ANY/g' | kubectl replace -n istio-system -f -


mvn install:install-file -DgroupId=grpc.cma.cimiss2.music -DartifactId=music-grpc-proto -Dversion=1.0.0.RELEASE -Dpackaging=jar -Dfile=music-grpc-proto-1.0.0.RELEASE.jar

kubectl set image -n grpc-demo deployment.apps/grpc-svc-demo-client-deployment grpc-svc-demo-client=registry.test.com/grpc-svc-demo-client:0.0.2 --record

kubectl exec -it sleep-8f795f47d-sfvth -n foo -c sleep -- curl -s  "grpc-svc-demo-client-service.grpc-demo.svc:8080/getSurfEleByTime?dataCode=SURF_CHN_MUL_MIN&times=20191113081500&elements=Station_Id_C,Datetime,TEM"

返回结果如下：
{"data": ["I0961,2019-11-13 08:15:00,14.3","S1813,2019-11-13 08:15:00,13.4","V6089,2019-11-13 08:15:00,999999.0"],"requestInfo": {"errorMessage": "query success!!","requestTime": "2020-07-10 07:19:53","responseTime": "2020-07-10 07:19:53","rowCount": 3,"colCount": 3}}

删除namespace时挂起
kubectl get namespace kiali-operator -o json > a.yaml
Remove kubernetes from the finalizers array
kubectl replace --raw "/api/v1/namespaces/kiali-operator/finalize" -f ./a.yaml

[root@localhost bitnami]# helm install -n cassandra cassdb ./cassandra
NAME: cassdb
LAST DEPLOYED: Wed Jul 15 16:00:16 2020
NAMESPACE: cassandra
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **

Cassandra can be accessed through the following URLs from within the cluster:

  - CQL: cassdb-cassandra.cassandra.svc.cluster.local:9042
  - Thrift: cassdb-cassandra.cassandra.svc.cluster.local:9160

To get your password run:

   export CASSANDRA_PASSWORD=$(kubectl get secret --namespace cassandra cassdb-cassandra -o jsonpath="{.data.cassandra-password}" | base64 --decode)

Check the cluster status by running:

   kubectl exec -it --namespace cassandra $(kubectl get pods --namespace cassandra -l app=cassandra,release=cassdb -o jsonpath='{.items[0].metadata.name}') nodetool status

To connect to your Cassandra cluster using CQL:

1. Run a Cassandra pod that you can use as a client:

   kubectl run --namespace cassandra cassdb-cassandra-client --rm --tty -i --restart='Never' \
   --env CASSANDRA_PASSWORD=$CASSANDRA_PASSWORD \
    \
   --image registry.test.com/bitnami/cassandra:3.11.6-debian-10-r131 -- bash

2. Connect using the cqlsh client:

   cqlsh -u cassandra -p $CASSANDRA_PASSWORD cassdb-cassandra

To connect to your database from outside the cluster execute the following commands:

   kubectl port-forward --namespace cassandra svc/cassdb-cassandra 9042:9042 &
   cqlsh -u cassandra -p $CASSANDRA_PASSWORD 127.0.0.1 9042
[root@localhost bitnami]# 

[root@localhost bitnami]# helm install -n es esdb ./elasticsearch
NAME: esdb
LAST DEPLOYED: Wed Jul 15 16:03:52 2020
NAMESPACE: es
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
-------------------------------------------------------------------------------
 WARNING

    Elasticsearch requires some changes in the kernel of the host machine to
    work as expected. If those values are not set in the underlying operating
    system, the ES containers fail to boot with ERROR messages.

    More information about these requirements can be found in the links below:

      https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html
      https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html

    This chart uses a privileged initContainer to change those settings in the Kernel
    by running: sysctl -w vm.max_map_count=262144 && sysctl -w fs.file-max=65536

** Please be patient while the chart is being deployed **

  Elasticsearch can be accessed within the cluster on port 9200 at esdb-elasticsearch-coordinating-only.es.svc.cluster.local

  To access from outside the cluster execute the following commands:

    kubectl port-forward --namespace es svc/esdb-elasticsearch-coordinating-only 9200:9200 &
    curl http://127.0.0.1:9200/
[root@localhost bitnami]# 

[root@localhost bitnami]# helm install -n grafana grafana ./grafana
NAME: grafana
LAST DEPLOYED: Wed Jul 15 16:17:40 2020
NAMESPACE: grafana
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **

1. Get the application URL by running these commands:
    echo "Browse to http://127.0.0.1:8080"
    kubectl port-forward svc/grafana 8080:3000 &

2. Get the admin credentials:

    echo "User: admin"
    echo "Password: $(kubectl get secret grafana-admin --namespace grafana -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
[root@localhost bitnami]# 

[root@localhost bitnami]# helm install -n kafka kafka ./kafka
NAME: kafka
LAST DEPLOYED: Wed Jul 15 17:16:27 2020
NAMESPACE: kafka
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **

Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:

    kafka.kafka.svc.cluster.local

Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:
    kafka-0.kafka-headless.kafka.svc.cluster.local

To create a pod that you can use as a Kafka client run the following commands:

    kubectl run kafka-client --restart='Never' --image registry.test.com/bitnami/kafka:2.5.0-debian-10-r78 --namespace kafka --command -- sleep infinity
    kubectl exec --tty -i kafka-client --namespace kafka -- bash

    PRODUCER:
        kafka-console-producer.sh \
            
            --broker-list kafka-0.kafka-headless.kafka.svc.cluster.local:9092, \
            --topic test

    CONSUMER:
        kafka-console-consumer.sh \
            
            --bootstrap-server kafka.kafka.svc.cluster.local:9092 \
            --topic test \
            --from-beginning
[root@localhost bitnami]#

Cassandra在Istio环境运行需修改容器中文件

CAN I RUN CASSANDRA INSIDE AN ISTIO MESH?
By default, Cassandra broadcasts the address it uses for binding (accepting connections) to other Cassandra nodes as its address. This is usually the pod IP address and works fine without a service mesh. However, with a service mesh this configuration does not work. Istio and other service meshes require localhost (127.0.0.1) to be the address for binding.

There are two configuration parameters to pay attention to: listen_address and broadcast_address. For running Cassandra in an Istio mesh, the listen_address parameter should be set to 127.0.0.1 and the broadcast_address parameter should be set to the pod IP address.

These configuration parameters are defined in cassandra.yaml in the Cassandra configuration directory (e.g. /etc/cassandra). There are various startup scripts (and yaml files) used for starting Cassandra and care should be given to how these parameters are set by these scripts. For example, some scripts used to configure and start Cassandra use the value of the environment variable CASSANDRA_LISTEN_ADDRESS for setting listen_address.

--------------------------------------------------------------------------------------------------------------------
FROM registry.test.com/bitnami/cassandra:3.11.6-debian-10-r131
RUN sed -i 's/cassandra_yaml_set "listen_address" "$host"/cassandra_yaml_set "listen_address" "127.0.0.1"/' /opt/bitnami/scripts/libcassandra.sh
RUN sed -i 's/cassandra_yaml_set "broadcast_address" "$CASSANDRA_BROADCAST_ADDRESS"/cassandra_yaml_set "broadcast_address" "$host"/' /opt/bitnami/scripts/libcassandra.sh

values.yaml中增加
istio: true

statefulset.yaml中增加
            {{- if .Values.istio }}
            - name: CASSANDRA_BROADCAST_ADDRESS
              value: {{ .Values.istio | quote }}
            {{- end }}
--------------------------------------------------------------------------------------------------------------------

kubectl edit -n istio-system configmap

bash <(curl -L https://kiali.io/getLatestKialiOperator) --accessible-namespaces '**' -oin 'registry.test.com/kiali/kiali-operator' -oiv latest -kin 'registry.test.com/kiali/kiali' -kiv 'v1.15' -ju 'http://simple-prod-query.tracing.svc.cluster.local:16686' -gu 'http://grafana.grafana.svc.cluster.local:3000'

安装jaeger
kubectl create ns tracing
kubectl apply -f jaegertracing.io_jaegers_crd.yaml
kubectl apply -n tracing -f service_account.yaml
kubectl apply -n tracing -f role.yaml
kubectl apply -n tracing -f role_binding.yaml
kubectl apply -n tracing -f operator.yaml

kubectl apply -n tracing -f deploy-one.yaml


