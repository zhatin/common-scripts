
修改/etc/hosts文件
10.224.47.161   master01
10.224.47.162   worker01
10.224.47.163   worker02
10.224.47.164   worker03

设置时区
timedatectl set-timezone Asia/Shanghai

设置主机名
hostnamectl set-hostname master01
hostnamectl set-hostname worker01
hostnamectl set-hostname worker02
hostnamectl set-hostname worker03

修改/etc/selinux/config
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
setenforce 0

关闭防火墙
systemctl disable firewalld
systemctl stop firewalld

设置内核参数
vi /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

sysctl --system

关闭交换分区
swapoff -a
sed -e '/swap/s/^/#/g' -i /etc/fstab

上传Docker离线安装包docker.tar.gz
安装Docker
rpm -Uvh --replacefiles --replacepkgs --nodeps --force *.rpm
systemctl enable docker
systemctl start docker

修改Docker参数
vi /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}

systemctl daemon-reload
systemctl restart docker

上传k8s离线安装包k8s.tar.gz
安装kubeadm
rpm -Uvh --replacefiles --replacepkgs --nodeps --force *.rpm

systemctl enable kubelet
systemctl start kubelet

导入镜像
docker load -i coredns.tar 
docker load -i etcd.tar 
docker load -i flannel.tar
docker load -i kube-apiserver.tar 
docker load -i kube-controller-manager.tar 
docker load -i kube-proxy.tar 
docker load -i kube-scheduler.tar 
docker load -i pause.tar

初始化master 
kubeadm init --pod-network-cidr=10.244.0.0/16
-----------------------------------------------------------
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.224.47.161:6443 --token pe6cd8.il4wx3aw5rcvtfhx \
    --discovery-token-ca-cert-hash sha256:5098c4dbec46eadb0800943ee8eb751e19e402e49a5d4ff3fb88eaf5105e0f80 
-----------------------------------------------------------

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

安装Flannel
kubectl apply -f kube-flannel.yml

加入Worker
kubeadm join 10.224.47.161:6443 --token pe6cd8.il4wx3aw5rcvtfhx \
    --discovery-token-ca-cert-hash sha256:5098c4dbec46eadb0800943ee8eb751e19e402e49a5d4ff3fb88eaf5105e0f80 

设置Worker的ROLES标签
kubectl label node worker01 node-role.kubernetes.io/worker=worker
kubectl label node worker02 node-role.kubernetes.io/worker=worker
kubectl label node worker03 node-role.kubernetes.io/worker=worker

------------------------------------------
[root@master01 ~]# kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
master01   Ready    master   15h   v1.18.2
worker01   Ready    worker   15h   v1.18.2
worker02   Ready    worker   15h   v1.18.2
worker03   Ready    worker   15h   v1.18.2
[root@master01 ~]# 
------------------------------------------

------------------------------------------
[root@master01 ~]# kubectl get pods -A -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE       NOMINATED NODE   READINESS GATES
kube-system   coredns-66bff467f8-2hht5           1/1     Running   0          16h   10.244.1.2      worker01   <none>           <none>
kube-system   coredns-66bff467f8-7zkx2           1/1     Running   0          16h   10.244.0.2      master01   <none>           <none>
kube-system   etcd-master01                      1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-apiserver-master01            1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-controller-manager-master01   1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-flannel-ds-amd64-hfnl4        1/1     Running   0          16h   10.224.47.163   worker02   <none>           <none>
kube-system   kube-flannel-ds-amd64-lc84v        1/1     Running   0          16h   10.224.47.164   worker03   <none>           <none>
kube-system   kube-flannel-ds-amd64-vdgf5        1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-flannel-ds-amd64-xd9j5        1/1     Running   0          16h   10.224.47.162   worker01   <none>           <none>
kube-system   kube-proxy-4gnrv                   1/1     Running   0          16h   10.224.47.162   worker01   <none>           <none>
kube-system   kube-proxy-6xfzt                   1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
kube-system   kube-proxy-kq2t8                   1/1     Running   0          16h   10.224.47.164   worker03   <none>           <none>
kube-system   kube-proxy-znxqx                   1/1     Running   0          16h   10.224.47.163   worker02   <none>           <none>
kube-system   kube-scheduler-master01            1/1     Running   0          16h   10.224.47.161   master01   <none>           <none>
[root@master01 ~]# 
------------------------------------------

上传Metallb安装包文件metallb.tar.gz
导入镜像文件

docker load -i controller.tar
docker load -i speaker.tar

修改参数
kubectl edit configmap -n kube-system kube-proxy

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
ipvs:
  strictARP: true
  
编写config.yaml
vi config.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 10.224.47.165-10.224.47.166

安装Metallb
kubectl apply -f namespace.yaml
kubectl apply -f metallb.yaml
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
kubectl apply -f config.yaml

编辑参数
kubectl edit configmap/config -n metallb-system

上传Glusterfs文件包gluster.tar.gz
导入镜像文件

docker load -i gluster.tar
docker load -i heketi.tar

在Workers上安装glusterfs-fuse
rpm -Uvh --replacefiles --replacepkgs --nodeps --force *.rpm

vi topology.json

{
  "clusters": [
    {
      "nodes": [
        {
          "node": {
            "hostnames": {
              "manage": [
                "worker01"
              ],
              "storage": [
                "10.224.47.162"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "worker02"
              ],
              "storage": [
                "10.224.47.163"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "worker03"
              ],
              "storage": [
                "10.224.47.164"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        }
      ]
    }
  ]
}		

编辑yaml文件，修改api版本和增加spec/selector

部署gluster
kubectl create namespace heketi

擦除磁盘
wipefs --all --force /dev/sdb

在worker上加载内核模块
modprobe dm_thin_pool

vi /etc/modules-load.d/glusterfs.conf

dm_thin_pool

如果出现Can't open /dev/sdb exclusively.  Mounted filesystem?
 echo 1 > /sys/block/sdb/device/delete

删除gk-deploy中--show-all参数后运行
./gk-deploy -g -v -n heketi --admin-key heketi-glusterfs -y 

----------------------------------------
Determining heketi service URL ... OK

heketi is now running and accessible via http://10.244.2.32:8080 . To run
administrative commands you can install 'heketi-cli' and use it as follows:

  # heketi-cli -s http://10.244.2.32:8080 --user admin --secret '<ADMIN_KEY>' cluster list

You can find it at https://github.com/heketi/heketi/releases . Alternatively,
use it from within the heketi pod:

  # /usr/bin/kubectl -n heketi exec -it <HEKETI_POD> -- heketi-cli -s http://localhost:8080 --user admin --secret '<ADMIN_KEY>' cluster list

For dynamic provisioning, create a StorageClass similar to this:

---
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: glusterfs-storage
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://10.244.2.32:8080"


Deployment complete!

[root@master01 deploy]# 
----------------------------------------

新建StorageClass
[root@master01 deploy]# vi storage-class.yaml
---
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: glusterfs-storage
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://10.244.2.32:8080"
  restuser: "admin"
  restuserkey: "heketi-glusterfs"

kubectl create -f storage-class.yaml

如果需要修改reseturl
kubectl replace -f storage-class.yaml --force 

[root@master01 deploy]# kubectl get storageclass
NAME                PROVISIONER               RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
glusterfs-storage   kubernetes.io/glusterfs   Delete          Immediate           false                  15s
[root@master01 deploy]#

新建PersistentVolumeClaim
vi persistent-volumeclaim.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: gluster-test
 annotations:
   volume.beta.kubernetes.io/storage-class: glusterfs-storage
spec:
 accessModes:
  - ReadWriteOnce
 resources:
   requests:
     storage: 5Gi

kubectl create -f persistent-volumeclaim.yaml 

persistentvolumeclaim/gluster-test created
[root@master01 deploy]# kubectl get pvc
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
gluster-test   Bound    pvc-957238b5-517f-4017-a55a-a321b1676486   5Gi        RWO            glusterfs-storage   8s
[root@master01 deploy]#
[root@master01 deploy]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS        REASON   AGE
pvc-957238b5-517f-4017-a55a-a321b1676486   5Gi        RWO            Delete           Bound    default/gluster-test   glusterfs-storage            47s
[root@master01 deploy]#

测试Gluster
vi nginx.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-test-pod
  labels:
    name: nginx-test-pod
spec:
  containers:
  - name: nginx-test-pod
    image: nginx
    imagePullPolicy: IfNotPresent
    ports:
    - name: web
      containerPort: 80
    volumeMounts:
    - name: gluster-vol-test
      mountPath: /usr/share/nginx/html
  volumes:
  - name: gluster-vol-test
    persistentVolumeClaim:
      claimName: gluster-test

kubectl create -f nginx.yaml

vi nginx-deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        imagePullPolicy: IfNotPresent
        ports:
        - name: web
          containerPort: 80
        volumeMounts:
        - name: gluster-vol-test
          mountPath: /usr/share/nginx/html
      volumes:
      - name: gluster-vol-test
        persistentVolumeClaim:
          claimName: gluster-test

kubectl create -f nginx-deploy.yaml

kubectl expose deployment nginx-deployment --port=80 --type=LoadBalancer

[root@master01 deploy]# kubectl get svc -o wide
NAME                                                     TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE     SELECTOR
details                                                  ClusterIP      10.98.113.165    <none>          9080/TCP       2d19h   app=details
glusterfs-dynamic-957238b5-517f-4017-a55a-a321b1676486   ClusterIP      10.105.118.147   <none>          1/TCP          57m     <none>
kubernetes                                               ClusterIP      10.96.0.1        <none>          443/TCP        3d20h   <none>
nginx-deployment                                         LoadBalancer   10.96.239.82     10.224.47.166   80:30811/TCP   3m27s   app=nginx
productpage                                              ClusterIP      10.96.221.151    <none>          9080/TCP       2d19h   app=productpage
ratings                                                  ClusterIP      10.100.183.15    <none>          9080/TCP       2d19h   app=ratings
reviews                                                  ClusterIP      10.100.220.182   <none>          9080/TCP       2d19h   app=reviews
[root@master01 deploy]#

[root@localhost ~]# curl http://10.224.47.166
Hello World from GlusterFS!!!
[root@localhost ~]#

[root@master01 deploy]# kubectl get pods                 
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-8556f9b6fd-52rmp       2/2     Running   2          2d19h
nginx-test-pod                    1/1     Running   0          13s
productpage-v1-559cb4c69f-wkbw5   2/2     Running   2          2d19h
ratings-v1-66bc766cf4-sbvf5       2/2     Running   2          2d19h
reviews-v1-bcf9667f9-4c2s5        2/2     Running   2          2d19h
reviews-v2-7846b549c-mxt5q        2/2     Running   2          2d19h
reviews-v3-68c569cb87-xs7lh       2/2     Running   2          2d19h
[root@master01 deploy]#

[root@master01 deploy]# kubectl exec -ti nginx-test-pod -- /bin/bash
root@nginx-test-pod:/# cd /usr/share/nginx/html/
root@nginx-test-pod:/usr/share/nginx/html# echo 'Hello World from GlusterFS!!!' > index.html
root@nginx-test-pod:/usr/share/nginx/html# ls
index.html
root@nginx-test-pod:/usr/share/nginx/html# exit
exit
[root@master01 deploy]#

上传Istio安装安装文件和镜像包istio-image.tar.gz
导入镜像文件

docker load -i citadel.tar 
docker load -i coredns-plugin.tar 
docker load -i galley.tar 
docker load -i grafana.tar 
docker load -i mixer.tar 
docker load -i node-agent-k8s.tar 
docker load -i pilot.tar 
docker load -i proxyv2.tar 
docker load -i sidecar_injector.tar 
docker load -i prometheus.tar
docker load -i jaeger.tar
docker load -i kiali.tar

istioctl operator init
kubectl create ns istio-system

istio profile dump demo
-------------------------------------
vi demo.yaml

apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  namespace: istio-system
  name: example-istiocontrolplane
spec:
  profile: demo
  
kubectl apply -f demo.yaml

--------------------------------------------------
[root@master01 ~]# kubectl get svc -n istio-system
NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                                                                                                                      AGE
grafana                     ClusterIP      10.106.125.123   <none>          3000/TCP                                                                                                                                     98m
istio-egressgateway         ClusterIP      10.105.226.46    <none>          80/TCP,443/TCP,15443/TCP                                                                                                                     98m
istio-ingressgateway        LoadBalancer   10.110.198.166   10.224.47.165   15020:30732/TCP,80:32155/TCP,443:32631/TCP,15029:31461/TCP,15030:30922/TCP,15031:31468/TCP,15032:31650/TCP,31400:31367/TCP,15443:30773/TCP   98m
istio-pilot                 ClusterIP      10.103.30.159    <none>          15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP                                                                                     98m
istiod                      ClusterIP      10.103.48.155    <none>          15012/TCP,443/TCP                                                                                                                            98m
jaeger-agent                ClusterIP      None             <none>          5775/UDP,6831/UDP,6832/UDP                                                                                                                   98m
jaeger-collector            ClusterIP      10.96.15.188     <none>          14267/TCP,14268/TCP,14250/TCP                                                                                                                98m
jaeger-collector-headless   ClusterIP      None             <none>          14250/TCP                                                                                                                                    98m
jaeger-query                ClusterIP      10.103.55.178    <none>          16686/TCP                                                                                                                                    98m
kiali                       ClusterIP      10.96.151.89     <none>          20001/TCP                                                                                                                                    98m
prometheus                  ClusterIP      10.103.41.21     <none>          9090/TCP                                                                                                                                     98m
tracing                     ClusterIP      10.105.195.80    <none>          80/TCP                                                                                                                                       98m
zipkin                      ClusterIP      10.109.240.158   <none>          9411/TCP                                                                                                                                     98m
[root@master01 ~]#
--------------------------------------------------
自动注入
kubectl label namespace default istio-injection=enabled

使用手动注入（因Flannel与Istio的兼容问题，自动注入失败）
istioctl kube-inject -f bookinfo.yaml | kubectl apply -f -

打开Kiali
istioctl dashboard kiali

安装MySQL

[root@master01 mysql]# echo -n "mysql-test-secret-passwd" | base64
bXlzcWwtdGVzdC1zZWNyZXQtcGFzc3dk
[root@master01 mysql]#

vi secret.yaml

---
apiVersion: v1
kind: Secret
metadata:
  name: mysql-test-secrets
type: Opaque
data:
  ROOT_PASSWORD: bXlzcWwtdGVzdC1zZWNyZXQtcGFzc3dk

kubectl apply -f secret.yaml

vi pvc.yaml

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-test-disk
  annotations:
    volume.beta.kubernetes.io/storage-class: glusterfs-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

kubectl apply -f pvc.yaml

vi deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-test-deployment
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql:5
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3306
          volumeMounts:
            - mountPath: "/var/lib/mysql"
              subPath: "mysql"
              name: mysql-data
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-test-secrets
                  key: ROOT_PASSWORD
      volumes:
        - name: mysql-data
          persistentVolumeClaim:
            claimName: mysql-test-disk

kubectl apply -f deploy.yaml

vi svc.yaml

---
apiVersion: v1
kind: Service
metadata:
  name: mysql-test-service
spec:
  selector:
    app: mysql
  ports:
  - protocol: TCP
    port: 3306
    targetPort: 3306

kubectl apply -f svc.yaml

安装Apollo
初始化数据库
kubectl exec -it mysql-test-deployment-5f6c7599d9-ldstq -- mysql -u root -pmysql-test-secret-passwd < apolloconfigdb.sql
kubectl exec -it mysql-test-deployment-5f6c7599d9-ldstq -- mysql -u root -pmysql-test-secret-passwd < apolloportaldb.sql

修改yaml后
kubectl apply -f service-apollo-config-server-dev.yaml
kubectl apply -f service-apollo-admin-server-dev.yaml
kubectl apply -f service-apollo-portal-server.yaml

部署demo
kubectl apply -f deploy.yaml，或者istioctl kube-inject -f deploy.yaml | kubectl apply -f - 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: apollo-demo-deployment
  labels:
    app: apollo-demo-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: apollo-demo-app
  template:
    metadata:
      labels:
        app: apollo-demo-app
    spec:
      containers:
      - name: apollo-demo-app
        image: apollo-demo:0.0.1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: apollo-demo-service
spec:
  selector:
    app: apollo-demo-app
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30008
  type: LoadBalancer

安装redis
上传gcr-redis.tar

kubectl create -f redis-master.yaml
kubectl create -f redis-sentinel-service.yaml
kubectl create -f redis-controller.yaml
kubectl create -f redis-sentinel-controller.yaml

安装minio

如果PVC无法删除
kubectl describe pvc PVC_NAME | grep Finalizers
kubectl patch pvc PVC_NAME -p '{"metadata":{"finalizers": []}}' --type=merge

安装Spinnaker

apiVersion: v1
kind: ConfigMap
metadata:
  name: halyard-config-map
data:
  config: |
    halyard:
      halconfig:
        directory: /Users/spinnaker/.hal
    spinnaker:
      config:
        input:
          endpoint: http://minio:9000
          accessKeyId: "minio"
          secretAccessKey: "minio123"
          bucket: halconfig
          enablePathStyleAccess: true
    secrets:
      vault:
        enabled: false
        url: https://vault.url
        path: example
        role: example
        authMethod: KUBERNETES


kubectl -n spinnaker apply -f halyard.yml

kubectl -n spinnaker exec -it halyard-0 -- /bin/bash

hal config provider kubernetes enable --no-validate
hal config provider kubernetes account add spinnaker   --provider-version v2   --only-spinnaker-managed true   --service-account true   --namespaces spinnaker --no-validate
hal config deploy edit   --type distributed   --account-name spinnaker   --location spinnaker --no-validate

hal config features edit --artifacts true --no-validate
hal config features edit --artifacts-rewrite true --no-validate
hal config artifact http enable --no-validate

hal config storage s3 edit \
--bucket spinnaker \
--endpoint http://minio.default:9000 \
--access-key-id minio \
--secret-access-key minio123 \
--path-style-access true \
--no-validate

hal config storage edit --type s3

mkdir -p /home/spinnaker/.hal/default/{profiles,service-settings}

vi /home/spinnaker/.hal/default/profiles/gate-local.yml

server:
  servlet:
    context-path: /api/v1

vi /home/spinnaker/.hal/default/service-settings/gate.yml

healthEndpoint: /api/v1/health

echo "spinnaker.s3.versioning: false" > /home/spinnaker/.hal/default/profiles/front50-local.yml

hal version bom 1.20.2 -q -o yaml

hal config version edit --version 1.20.2 --no-validate

docker pull docker.io/armory/clouddriver:2.19.8
docker pull docker.io/armory/echo:2.19.8
docker pull docker.io/armory/fiat:2.19.6
docker pull docker.io/armory/front50:2.19.6
docker pull docker.io/armory/gate:2.19.5
docker pull docker.io/armory/igor:2.19.6
docker pull docker.io/armory/orca:2.19.9
docker pull docker.io/armory/rosco:2.19.6
docker pull docker.io/armory/deck:2.19.7
docker pull docker.io/armory/dinghy:2.19.5
docker pull docker.io/armory/terraformer:1.0.6
docker pull docker.io/armory/kayenta:2.19.5
docker pull docker.io/armory/monitoring-daemon:0.16.1-7d506f0-rc1
#docker pull docker.io/armory/monitoring-third-party:0.16.1-7d506f0-rc1（not exist）
docker pull docker.io/armory/redis:2


version: 2.19.8-rc.1
timestamp: "2020-04-22 01:49:47"
services:
  clouddriver:
    commit: 2bd55acb
    version: 2.19.8
  echo:
    commit: 43e1966a
    version: 2.19.8
  fiat:
    commit: e7d5efa3
    version: 2.19.6
  front50:
    commit: 32cc7a7c
    version: 2.19.6
  gate:
    commit: 5ea58df0
    version: 2.19.5
  igor:
    commit: 67f5ae20
    version: 2.19.6
  orca:
    commit: be0f8e7a
    version: 2.19.9
  rosco:
    commit: e168a011
    version: 2.19.6
  deck:
    commit: 4f6b2719
    version: 2.19.7
  dinghy:
    commit: ef444037
    version: 2.19.5
  terraformer:
    commit: f3edd3da
    version: 1.0.6
  kayenta:
    commit: fa1521ae
    version: 2.19.5
  monitoring-daemon:
    version: 0.16.1-7d506f0-rc1
  monitoring-third-party:
    version: 0.16.1-7d506f0-rc1
dependencies:
  redis:
    version: 2:2.8.4-2
artifactSources:
  dockerRegistry: docker.io/armory

hal deploy apply

openssl req \
 -newkey rsa:2048 \
 -nodes -sha256 \
 -x509 -days 365 \
 -keyout registry.key \
 -out registry.crt


kubectl create secret tls registry-tls \
    --key registry.key \
    --cert registry.crt


docker run -p 8084:8084 -p 9000:9000 \
    --name halyard --rm \
    -v ~/.hal:/home/spinnaker/.hal \
    -d \
    armory/halyard-armory:1.9.1

docker exec -it halyard bash

kubectl get storageclass
kubectl patch storageclass glusterfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

services:
  echo:
    version: 2.12.1-20200507020017
    commit: 7b04fb3d80dacba05dad29b754615a546168c8cc
  clouddriver:
    version: 6.8.2-20200518071229
    commit: f6709294361ac4459bf63f2064d43d699e4630e5
  deck:
    version: 3.1.1-20200518071229
    commit: e0f33be10ce4d2cf79ea7f66f31a6059ba3bc7ab
  fiat:
    version: 1.11.0-20200428134356
    commit: c88008c3a1359c445eee8cff23184e5484e472a9
  front50:
    version: 0.23.1-20200509020016
    commit: 4cbc3b136ac7cc200f7a58040e7d352390d38a31
  gate:
    version: 1.16.0-20200501020017
    commit: c9975aa819f06115a7a27ddf8156b487712cd980
  igor:
    version: 1.10.0-20200428134356
    commit: 1c1d7d69689b46cc6483ef23281a810ea458e51c
  kayenta:
    version: 0.15.1-20200509020016
    commit: f3af8422684b1c95eced83020e14c8e27ed7b466
  orca:
    version: 2.14.0-20200502020017
    commit: e2b32d9cc794b30048f502728f8126eae227eeec
  rosco:
    version: 0.19.0-20200428134356
    commit: 32481f1ed3532ac802cb9dc9cff719a480f63a20
  defaultArtifact: {}
  monitoring-third-party:
    version: 0.17.0-20200501020017
    commit: 226f54ffe8a49df95af290bcd57aa0c2f225ed36
  monitoring-daemon:
    version: 0.17.0-20200501020017
    commit: 226f54ffe8a49df95af290bcd57aa0c2f225ed36
dependencies:
  redis:
    version: 2:2.8.4-2
  consul:
    version: 0.7.5
  vault:
    version: 0.7.0
artifactSources:
  debianRepository: https://dl.bintray.com/spinnaker-releases/debians
  dockerRegistry: gcr.io/spinnaker-marketplace
  googleImageProject: marketplace-spinnaker-release
  gitPrefix: https://github.com/spinnaker

docker pull gcr.io/spinnaker-marketplace/clouddriver:6.8.2-20200518071229
docker pull gcr.io/spinnaker-marketplace/echo:2.12.1-20200507020017
docker pull gcr.io/spinnaker-marketplace/fiat:1.11.0-20200428134356
docker pull gcr.io/spinnaker-marketplace/front50:0.23.1-20200509020016
docker pull gcr.io/spinnaker-marketplace/gate:1.16.0-20200501020017
docker pull gcr.io/spinnaker-marketplace/igor:1.10.0-20200428134356
docker pull gcr.io/spinnaker-marketplace/orca:2.14.0-20200502020017
docker pull gcr.io/spinnaker-marketplace/rosco:0.19.0-20200428134356
docker pull gcr.io/spinnaker-marketplace/deck:3.1.1-20200518071229
docker pull gcr.io/spinnaker-marketplace/kayenta:0.15.1-20200509020016
docker pull gcr.io/spinnaker-marketplace/monitoring-daemon:0.17.0-20200501020017
docker pull gcr.io/spinnaker-marketplace/monitoring-third-party:0.17.0-20200501020017
docker pull gcr.io/kubernetes-spinnaker/redis-cluster:v2
docker pull gcr.io/spinnaker-marketplace/consul:0.7.5
docker pull gcr.io/spinnaker-marketplace/vault:0.7.0

docker save gcr.io/spinnaker-marketplace/clouddriver:6.8.2-20200518071229 > clouddriver.tar
docker save gcr.io/spinnaker-marketplace/echo:2.12.1-20200507020017 > echo.tar
docker save gcr.io/spinnaker-marketplace/fiat:1.11.0-20200428134356 > fiat.tar
docker save gcr.io/spinnaker-marketplace/front50:0.23.1-20200509020016 > front50.tar
docker save gcr.io/spinnaker-marketplace/gate:1.16.0-20200501020017 > gate.tar
docker save gcr.io/spinnaker-marketplace/igor:1.10.0-20200428134356 > igor.tar
docker save gcr.io/spinnaker-marketplace/orca:2.14.0-20200502020017 > orca.tar
docker save gcr.io/spinnaker-marketplace/rosco:0.19.0-20200428134356 > rosco.tar
docker save gcr.io/spinnaker-marketplace/deck:3.1.1-20200518071229 > deck.tar
docker save gcr.io/spinnaker-marketplace/kayenta:0.15.1-20200509020016 > kayenta.tar
docker save gcr.io/kubernetes-spinnaker/redis-cluster:v2 > redis.tar

docker load -i clouddriver.tar
docker load -i echo.tar
docker load -i fiat.tar
docker load -i front50.tar
docker load -i gate.tar
docker load -i igor.tar
docker load -i orca.tar
docker load -i rosco.tar
docker load -i deck.tar
docker load -i kayenta.tar
docker load -i redis.tar


kubectl delete svc spin-clouddriver -n spinnaker 
kubectl delete svc spin-deck -n spinnaker 
kubectl delete svc spin-echo -n spinnaker 
kubectl delete svc spin-front50 -n spinnaker 
kubectl delete svc spin-gate -n spinnaker 
kubectl delete svc spin-orca -n spinnaker 
kubectl delete svc spin-redis -n spinnaker 
kubectl delete svc spin-rosco -n spinnaker 

kubectl delete deploy spin-clouddriver -n spinnaker 
kubectl delete deploy spin-deck -n spinnaker 
kubectl delete deploy spin-echo -n spinnaker 
kubectl delete deploy spin-front50 -n spinnaker 
kubectl delete deploy spin-gate -n spinnaker 
kubectl delete deploy spin-orca -n spinnaker 
kubectl delete deploy spin-redis -n spinnaker 
kubectl delete deploy spin-rosco -n spinnaker 

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: spin-ingress
  labels:
    app: spin
    cluster: spin-ingress
  annotations:
    kubernetes.io/ingress.class: istio
spec:
  rules:
  - 
    # host: spin.k8s.test
    # ^ If we have other things running in our cluster, we should uncomment this line and specify a valid DNS name
    http:
      paths:
      - backend:
          serviceName: spin-deck
          servicePort: 9000
        path: /
      - backend:
          serviceName: spin-gate
          servicePort: 8084
        path: /api/v1

kubectl -n spinnaker expose service spin-gate --type LoadBalancer \
  --port 80,443 \
  --target-port 8084 \
  --name spin-gate-public

kubectl -n spinnaker expose service spin-deck --type LoadBalancer \
  --port 80,443 \
  --target-port 9000 \
  --name spin-deck-public

kubectl -n spinnaker get svc spin-gate-public -o jsonpath='{.status.loadBalancer.ingress[0].ip}'
kubectl -n spinnaker get svc spin-gate-public -o jsonpath='{.status.loadBalancer.ingress[0].ip}'

hal config security api edit --override-base-url http://10.224.47.169
hal config security ui edit --override-base-url http://10.224.47.170

升级Istio 1.6.0

docker pull quay.io/kiali/kiali:v1.18
docker pull docker.io/prom/prometheus:v2.15.1
docker pull docker.io/istio/proxyv2:1.6.0
docker pull docker.io/jaegertracing/all-in-one:1.16
docker pull docker.io/istio/pilot:1.6.0
docker pull grafana/grafana:6.5.2
docker pull docker.io/istio/mixer:1.6.0
docker pull docker.io/istio/install-cni:1.6.0
docker pull docker.io/istio/operator:1.6.0

docker save quay.io/kiali/kiali:v1.18 > kiali.tar
docker save docker.io/prom/prometheus:v2.15.1 > prometheus.tar
docker save docker.io/istio/proxyv2:1.6.0 > proxyv2.tar
docker save docker.io/jaegertracing/all-in-one:1.16 > jaeger.tar
docker save docker.io/istio/pilot:1.6.0 > pilot.tar
docker save grafana/grafana:6.5.2 > grafana.tar
docker save docker.io/istio/mixer:1.6.0 > mixer.tar
docker save docker.io/istio/install-cni:1.6.0 > install-cni.tar
docker save docker.io/istio/operator:1.6.0 > operator.tar

docker load -i kiali.tar
docker load -i prometheus.tar
docker load -i proxyv2.tar
docker load -i jaeger.tar
docker load -i pilot.tar
docker load -i grafana.tar
docker load -i mixer.tar
docker load -i install-cni.tar
docker load -i operator.tar

集群二
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.224.47.165:6443 --token eh1q4b.ynau7ab2dp535qfr \
    --discovery-token-ca-cert-hash sha256:b70e52e1994bf6c0c40caaea6ba67c518528132cf345a330e7da8beb4534bd26 
[root@masterb01 rpms]# 
